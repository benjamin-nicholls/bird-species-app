{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Gi1n4qIK-LCS","outputId":"c7020fec-9d79-407a-c140-16636a316737"},"outputs":[],"source":["'''\n","conda create --name project python=3.9\n","pip install hugsvision=0.75.5\n","pip install ipykernel\n","pip install tensorboard\n","conda install pytorch=2.0.0 torchvision=0.15.0 torchaudio=2.0.0 pytorch-cuda=11.7 -c pytorch -c nvidia\n","pip install wrapt\n","pip install chardet\n","pip install --upgrade accelerate\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9TIgfpE-LCS","outputId":"4910b3b9-72e2-45df-bbbc-3ee13a64d14d"},"outputs":[],"source":["# https://github.com/qanastek/HugsVision/blob/main/recipes/kvasir_v2/binary_classification/Kvasir_v2_Image_Classifier.ipynb\n","import hugsvision\n","print(hugsvision.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dr1jectW-LCT","outputId":"078fa3d4-901d-4c19-9d34-789abbe861eb"},"outputs":[],"source":["from hugsvision.dataio.VisionDataset import VisionDataset\n","\n","# test will be overwritten. Ratio of 0.00 so all images in train folder are used for training.\n","# hugsvision.dataio.VisionDataset.VisionDataset splitDatasets is editted (tabulation).\n","train, test, id2label, label2id = VisionDataset.fromImageFolder(\n","\t\"./archive/train/\",\n","\ttest_ratio   = 0.00,\n","\tbalanced     = True,\n","\taugmentation = True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Only overwrite test variable. Ratio of 1.00 unbalanced to ensure all images go to testing.\n","train2, test, id2label2, label2id2 = VisionDataset.fromImageFolder(\n","\t\"./archive/test/\",\n","\ttest_ratio   = 1.00,\n","\tbalanced     = False,\n","\taugmentation = True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from hugsvision.nnet.VisionClassifierTrainer import VisionClassifierTrainer\n","from transformers import ViTFeatureExtractor, ViTForImageClassification\n","\n","# Train downstream from previously trained model on kvasir v2 dataset.\n","#model_path = './out/MyKvasirV2Model/1_2023-02-01-13-17-42/model/'\n","model_path = 'google/vit-base-patch16-224-in21k'\n","trainer = VisionClassifierTrainer(\n","\tmodel_name   = \"model-ds\",\n","\ttrain      \t = train,\n","\ttest      \t = test,\n","\toutput_dir   = \"./out/\",\n","\tmax_epochs   = 1,      # 1 default\n","\tbatch_size   = 32,     # 32 default\n","\tlr \t\t     = 2e-5,   # 2e-5 default\n","\tfp16\t     = False,  # False default\n","\tmodel = ViTForImageClassification.from_pretrained(\n","\t    model_path,\n","\t    num_labels = len(label2id),\n","\t    label2id   = label2id,\n","\t    id2label   = id2label,\n","        ignore_mismatched_sizes=True\n","\t),\n","\tfeature_extractor = ViTFeatureExtractor.from_pretrained(\n","\t\tmodel_path,\n","\t),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","hidden_size (`int`, *optional*, defaults to 768):\n","    Dimensionality of the encoder layers and the pooler layer.\n","num_hidden_layers (`int`, *optional*, defaults to 12):\n","    Number of hidden layers in the Transformer encoder.\n","num_attention_heads (`int`, *optional*, defaults to 12):\n","    Number of attention heads for each attention layer in the Transformer encoder.\n","intermediate_size (`int`, *optional*, defaults to 3072):\n","    Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n","hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n","    The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n","    `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n","hidden_dropout_prob (`float`, *optional*, defaults to 0.1):\n","    The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n","attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):\n","    The dropout ratio for the attention probabilities.\n","initializer_range (`float`, *optional*, defaults to 0.02):\n","    The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n","layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n","    The epsilon used by the layer normalization layers.\n","image_size (`int`, *optional*, defaults to `224`):\n","    The size (resolution) of each image.\n","patch_size (`int`, *optional*, defaults to `16`):\n","    The size (resolution) of each patch.\n","num_channels (`int`, *optional*, defaults to `3`):\n","    The number of input channels.\n","qkv_bias (`bool`, *optional*, defaults to `True`):\n","    Whether to add a bias to the queries, keys and values.\n","encoder_stride (`int`, `optional`, defaults to 16):\n","    Factor to increase the spatial resolution by in the decoder head for masked image modeling.\n","'''\n","from transformers import ViTConfig\n","\n","config = ViTConfig(\n","    hidden_size = 768,  \t\t# default 768\n","\tnum_hidden_layers = 12,  \t# default 12\n","\tnum_attention_heads = 4,  \t# default 12\n","\tintermediate_size = 3072,  \t# default 3072\n","\thidden_act = \"gelu\",  \t\t# default \"gelu\"\n","\thidden_dropout_prob = 0.0,  # default 0.0\n","\tattention_probs_dropout_prob = 0.0,  # default 0.0\n","\tinitializer_range = 0.02,   # default 0.02\n","\tlayer_norm_eps = 1e-12,  \t# default 1e-12\n","\timage_size = 224,  \t\t\t# default 224\n","\tpatch_size = 16,  \t\t\t# default 16\n","\tnum_channels = 3,  \t\t\t# default 3\n","\tqkv_bias = True,  \t\t\t# default True\n","\tencoder_stride = 16,  \t\t# default 16\n","\tnum_labels = len(label2id),\n","\tlabel2id   = label2id,\n","\tid2label   = id2label,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from hugsvision.nnet.VisionClassifierTrainer import VisionClassifierTrainer\n","from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTConfig\n","\n","# Train downstream from previously trained model on kvasir v2 dataset.\n","# kvasir_model_path = './out/MyKvasirV2Model/1_2023-02-01-13-17-42/model/'\n","trainer = VisionClassifierTrainer(\n","\tmodel_name   = \"model-s\",\n","\ttrain      \t = train,\n","\ttest      \t = test,\n","\toutput_dir   = \"./out/\",\n","\tmax_epochs   = 2,      # 1 default\n","\tbatch_size   = 32,     # 32 default\n","\tlr \t\t     = 2e-4,   # 2e-5 default\n","\tfp16\t     = False,  # false default\n","\tmodel = ViTForImageClassification(\n","\t\tconfig\n","\t),\n","\tfeature_extractor = ViTFeatureExtractor()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make a prediction.\n","from transformers import ViTFeatureExtractor, ViTForImageClassification\n","from hugsvision.inference.VisionClassifierInference import VisionClassifierInference\n","\n","path = './out/model/1/model/'\n","img = './albatross.png'\n","\n","classifier = VisionClassifierInference(\n","    feature_extractor = ViTFeatureExtractor.from_pretrained(path),\n","    model = ViTForImageClassification.from_pretrained(path),\n",")\n","\n","label = classifier.predict(img_path=img)\n","print('Predicted class:', label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyp, ref = trainer.evaluate_f1_score()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loop through all images and test for accuracy.\n","import os\n","from tqdm import tqdm\n","\n","count = 0\n","correct_count = 0\n","wrong_count = 0\n","\n","# get the path/directory\n","test_dir = \"./archive/valid/\"\n","for species in tqdm(os.listdir(test_dir)):\n","    for image in os.listdir(os.path.join(test_dir, species)):\n","        img = os.path.join(test_dir, species, image)\n","        label = classifier.predict(img_path=img)\n","        count += 1\n","        if (label == species): correct_count += 1\n","        else: wrong_count +=1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'Number correct = {correct_count}')\n","print(f'Number wrong = {wrong_count}')\n","print(f'Accuracy = {correct_count / count}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"project","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"e86aa4c9a0eba0b9f550c1c0cb347e5f78da4f2328baa1fcd40eab9f138ab846"}}},"nbformat":4,"nbformat_minor":0}
